import requests
import datetime
import logging
import threading
import time
import sqlite3
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from app.core.config import cfg
from app.core.database import DB_PATH

# åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger("uvicorn")

class CalendarService:
    def __init__(self):
        # å†…å­˜ç¼“å­˜ç»“æ„: { offset: {'data': ..., 'time': timestamp} }
        self._cache = {} 
        self._cache_lock = threading.Lock()
        
        # ğŸ”¥ å¯åŠ¨åå°å®ˆæŠ¤çº¿ç¨‹ï¼šå®šæ—¶æ‰§è¡Œå…¨é‡åŒæ­¥
        self._start_background_sync()

    def _start_background_sync(self):
        """
        åå°ç‹¬ç«‹çº¿ç¨‹ï¼šæ¯éš” 12 å°æ—¶è‡ªåŠ¨æ‹‰å– TMDB æ’æœŸå¹¶è½ç›˜ã€‚
        é˜²æ­¢ç”¨æˆ·åœ¨æœåŠ¡å™¨é‡å¯æˆ–é•¿æ—¶é—´æœªè®¿é—®åï¼Œé¦–æ¬¡æ‰“å¼€é¡µé¢åŠ è½½è¿‡æ…¢ã€‚
        """
        def sync_task():
            # å»¶è¿Ÿ 60 ç§’å¯åŠ¨ï¼Œç¡®ä¿ç³»ç»Ÿæ ¸å¿ƒç»„ä»¶ï¼ˆå¦‚æ•°æ®åº“ã€ç½‘ç»œä»£ç†ï¼‰å·²å°±ç»ª
            time.sleep(60)
            while True:
                try:
                    logger.info("ğŸ”„ [å®šæ—¶ä»»åŠ¡] å¼€å§‹åœ¨åå°è‡ªåŠ¨åˆ·æ–°è¿½å‰§æ—¥å†ç¼“å­˜...")
                    # å¼ºåˆ¶åŒæ­¥æœ¬å‘¨ (0) å’Œ ä¸‹å‘¨ (1) çš„æ•°æ®
                    self.get_weekly_calendar(force_refresh=True, week_offset=0)
                    self.get_weekly_calendar(force_refresh=True, week_offset=1)
                    logger.info("âœ… [å®šæ—¶ä»»åŠ¡] è¿½å‰§æ—¥å†åå°æ›´æ–°æˆåŠŸï¼Œæ•°æ®å·²æŒä¹…åŒ–è‡³ SQLiteã€‚")
                except Exception as e:
                    logger.error(f"âŒ [å®šæ—¶ä»»åŠ¡] åå°åŒæ­¥æ—¥å†å¤±è´¥: {e}")
                
                # ä¼‘çœ  12 å°æ—¶ (43200ç§’)
                time.sleep(43200)
        
        # daemon=True ç¡®ä¿ä¸»è¿›ç¨‹é€€å‡ºæ—¶çº¿ç¨‹èƒ½æ­£å¸¸é”€æ¯
        t = threading.Thread(target=sync_task, daemon=True)
        t.start()

    def _get_proxies(self):
        """è·å–å…¨å±€ä»£ç†é…ç½®ï¼Œç”¨äº TMDB è¯·æ±‚"""
        proxy = cfg.get("proxy_url")
        if proxy:
            return {"http": proxy, "https": proxy}
        return None

    def mark_episode_ready(self, series_id, season, episode):
        """
        Webhook è”åŠ¨æ¥å£ï¼šå½“ Emby æœ‰æ–°å‰§é›†å…¥åº“æ—¶è¢«è°ƒç”¨ã€‚
        ç›´æ¥ä¿®æ”¹æœ¬åœ°æ•°æ®åº“çŠ¶æ€ï¼Œå®ç°çº¢ç¯å˜ç»¿ç¯çš„å®æ—¶æ„Ÿã€‚
        """
        try:
            conn = sqlite3.connect(DB_PATH)
            c = conn.cursor()
            # æ ¹æ®ç³»åˆ—IDã€å­£ã€é›† ç²¾å‡†æ›´æ–°çŠ¶æ€ä¸º ready
            c.execute('''UPDATE tv_calendar_cache 
                         SET status = 'ready' 
                         WHERE series_id = ? AND season = ? AND episode = ?''', 
                      (series_id, season, episode))
            conn.commit()
            conn.close()
            
            # æ¸…ç†å†…å­˜ç¼“å­˜ï¼Œç¡®ä¿ä¸‹æ¬¡åˆ·æ–°é¡µé¢æ—¶è¯»åˆ°æœ€æ–°çŠ¶æ€
            with self._cache_lock:
                self._cache.clear()
            logger.info(f"ğŸŸ¢ [æ—¥å†è”åŠ¨] Webhook è§¦å‘æˆåŠŸï¼Œå·²ç‚¹äº®ç»¿ç¯: SeriesId={series_id} S{season}E{episode}")
        except Exception as e:
            logger.error(f"âŒ æ—¥å†çŠ¶æ€æ›´æ–°å¤±è´¥: {e}")

    def get_weekly_calendar(self, force_refresh=False, week_offset=0):
        """
        æ ¸å¿ƒæ–¹æ³•ï¼šè·å–å‘¨å†æ•°æ®
        é€»è¾‘æµï¼šå†…å­˜ç¼“å­˜ -> æœ¬åœ° SQLite ç¼“å­˜ -> TMDB API (å¼‚æ­¥æŠ“å–)
        """
        now = time.time()
        # ç¼“å­˜ç”Ÿå­˜æ—¶é—´ï¼Œé»˜è®¤ 24 å°æ—¶
        cache_ttl = int(cfg.get("calendar_cache_ttl") or 86400)

        # 1. ç¬¬ä¸€å±‚é˜²å¾¡ï¼šæ£€æŸ¥å†…å­˜äºŒçº§ç¼“å­˜
        if not force_refresh:
            with self._cache_lock:
                cached_item = self._cache.get(week_offset)
                if cached_item and (now - cached_item['time'] < cache_ttl):
                    return cached_item['data']

        api_key = cfg.get("tmdb_api_key")
        if not api_key:
            return {"error": "æœªé…ç½® TMDB API Keyï¼Œè¯·åœ¨è®¾ç½®ä¸­é…ç½®"}

        # 2. è®¡ç®—ç›®æ ‡å‘¨çš„æ—¥æœŸèŒƒå›´
        target_date = datetime.date.today() + datetime.timedelta(weeks=week_offset)
        start_of_week = target_date - datetime.timedelta(days=target_date.weekday())
        end_of_week = start_of_week + datetime.timedelta(days=6)
        
        # 3. è·å–æ­£åœ¨è¿è½½çš„å‰§é›†
        continuing_series = self._get_emby_continuing_series()
        if not continuing_series:
            return {"days": []}

        # 4. ç¬¬äºŒå±‚é˜²å¾¡ï¼šä»æœ¬åœ° SQLite è·å–è¿™ä¸€å‘¨çš„ç¼“å­˜æ•°æ®
        week_data = {i: [] for i in range(7)}
        start_date_str = start_of_week.strftime("%Y-%m-%d")
        end_date_str = end_of_week.strftime("%Y-%m-%d")
        
        has_db_data = False
        try:
            conn = sqlite3.connect(DB_PATH)
            c = conn.cursor()
            c.execute("SELECT status, data_json FROM tv_calendar_cache WHERE air_date >= ? AND air_date <= ?", 
                      (start_date_str, end_date_str))
            rows = c.fetchall()
            
            # åªæœ‰åœ¨éå¼ºåˆ¶åˆ·æ–°ä¸”æœ¬åœ°æœ‰æ•°æ®æ—¶ï¼Œæ‰ç›´æ¥ä½¿ç”¨ DB æ•°æ®
            if rows and not force_refresh:
                has_db_data = True
                for row in rows:
                    db_status = row[0]
                    item_data = json.loads(row[1])
                    # ç”¨æœ€æ–°çš„ DB çŠ¶æ€ï¼ˆå¯èƒ½è¢« Webhook ä¿®æ”¹è¿‡ï¼‰è¦†ç›– JSON é‡Œçš„åŸå§‹çŠ¶æ€
                    item_data["status"] = db_status
                    
                    try:
                        air_date_obj = datetime.datetime.strptime(item_data["air_date"], "%Y-%m-%d").date()
                        day_index = (air_date_obj - start_of_week).days
                        if 0 <= day_index <= 6:
                            week_data[day_index].append(item_data)
                    except: continue
            conn.close()
        except Exception as e:
            logger.error(f"SQLite è¯»å–å¼‚å¸¸: {e}")

        # 5. ç¬¬ä¸‰å±‚é€»è¾‘ï¼šå¦‚æœæœ¬åœ°æ— æ•°æ®æˆ–å¼ºåˆ¶åˆ·æ–°ï¼Œæ‰§è¡Œå¼‚æ­¥æŠ“å–
        if not has_db_data or force_refresh:
            week_data = {i: [] for i in range(7)} # é‡ç½®ç»“æœé›†
            proxies = self._get_proxies()
            
            with ThreadPoolExecutor(max_workers=20) as executor:
                future_to_series = {
                    executor.submit(self._fetch_series_status, s, api_key, start_of_week, end_of_week, proxies): s 
                    for s in continuing_series
                }
                
                for future in as_completed(future_to_series):
                    try:
                        results = future.result()
                        if results:
                            for item in results:
                                idx = item['day_index']
                                if 0 <= idx <= 6:
                                    week_data[idx].append(item['data'])
                    except Exception as e:
                        logger.error(f"TMDB Fetcher Task Error: {e}")
            
            # ğŸ”¥ æ•°æ®æŒä¹…åŒ–ï¼šå°†æ–°æŠ“å–çš„æ•°æ®å­˜å…¥ SQLite
            try:
                conn = sqlite3.connect(DB_PATH)
                c = conn.cursor()
                for i in range(7):
                    for data_dict in week_data[i]:
                        s_id = data_dict.get("series_id")
                        sn = data_dict.get("season")
                        en = data_dict.get("episode")
                        air_d = data_dict.get("air_date")
                        stat = data_dict.get("status")
                        
                        if s_id and sn is not None and en is not None:
                            id_key = f"{s_id}_{sn}_{en}"
                            c.execute('''INSERT OR REPLACE INTO tv_calendar_cache 
                                         (id, series_id, season, episode, air_date, status, data_json) 
                                         VALUES (?, ?, ?, ?, ?, ?, ?)''', 
                                      (id_key, s_id, sn, en, air_d, stat, json.dumps(data_dict)))
                conn.commit()
                conn.close()
            except Exception as e:
                logger.error(f"SQLite å†™å…¥å¼‚å¸¸: {e}")

        # 6. æ™ºèƒ½å»é‡ä¸å¤šé›†èšåˆé€»è¾‘ (ä¾‹å¦‚ S01E01-E02)
        for i in range(7):
            raw_items = week_data[i]
            if not raw_items: continue

            grouped = {}
            for item in raw_items:
                key = (item.get('tmdb_id') or item['series_id'], item['season'])
                if key not in grouped:
                    grouped[key] = []
                grouped[key].append(item)
            
            merged_items = []
            for key, group in grouped.items():
                # æ’åºä¿è¯è¿å·é›†æ•°èƒ½æ­£ç¡®å±•ç¤º
                sorted_eps = sorted(group, key=lambda x: x['episode'])
                if not sorted_eps: continue

                if len(sorted_eps) == 1:
                    merged_items.append(sorted_eps[0])
                else:
                    first, last = sorted_eps[0], sorted_eps[-1]
                    merged = first.copy()
                    merged['episode'] = f"{first['episode']}-{last['episode']}"
                    merged['ep_name'] = None 
                    # åªè¦æœ‰ä¸€é›†ç¼ºå¤±ï¼Œæ•´ä½“å°±æ ‡è®°ä¸ºç¼ºå¤±
                    statuses = [x['status'] for x in sorted_eps]
                    if 'missing' in statuses: merged['status'] = 'missing'
                    elif 'ready' in statuses: merged['status'] = 'ready'
                    else: merged['status'] = 'upcoming'
                    merged_items.append(merged)
            
            # æŒ‰é›†æ•°æ’åºå¹¶æ›´æ–°ç»“æœé›†
            week_data[i] = sorted(merged_items, key=lambda x: str(x['episode']))

        # 7. æœ€ç»ˆå“åº”æ ¼å¼åŒ–
        final_days = []
        week_dates = [start_of_week + datetime.timedelta(days=i) for i in range(7)]
        today_real = datetime.date.today()
        
        for i in range(7):
            final_days.append({
                "date": week_dates[i].strftime("%Y-%m-%d"),
                "weekday_cn": ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"][i],
                "is_today": week_dates[i] == today_real, 
                "items": week_data[i]
            })
        
        # è·å– Emby åŸºæœ¬åœ°å€
        emby_url = (cfg.get("emby_public_url") or cfg.get("emby_host") or "").rstrip('/')

        # åŠ¨æ€è·å–å½“å‰ Emby çš„ ServerId ç”¨äºå‰ç«¯è·³è½¬æ’­æ”¾
        server_id = ""
        try:
            key, host = cfg.get("emby_api_key"), cfg.get("emby_host")
            sys_res = requests.get(f"{host}/emby/System/Info?api_key={key}", timeout=5)
            if sys_res.status_code == 200:
                server_id = sys_res.json().get("Id", "")
        except: pass

        result = {
            "days": final_days, 
            "emby_url": emby_url,
            "server_id": server_id,
            "date_range": f"{start_of_week.strftime('%m/%d')} - {end_of_week.strftime('%m/%d')}",
            "current_ttl": cache_ttl 
        }
        
        # å†™å…¥å†…å­˜ç¼“å­˜
        with self._cache_lock:
            self._cache[week_offset] = {'data': result, 'time': now}
            
        return result

    def _get_emby_continuing_series(self):
        """ä» Emby è·å–æ‰€æœ‰çŠ¶æ€ä¸º Continuing çš„å‰§é›†"""
        key, host = cfg.get("emby_api_key"), cfg.get("emby_host")
        user_id = self._get_admin_id()
        if not key or not host or not user_id: return []

        try:
            url = f"{host}/emby/Users/{user_id}/Items"
            params = {
                "IncludeItemTypes": "Series",
                "Recursive": "true",
                "Fields": "ProviderIds,Status",
                "IsVirtual": "false",
                "api_key": key
            }
            res = requests.get(url, params=params, timeout=10)
            if res.status_code == 200:
                items = res.json().get("Items", [])
                return [i for i in items if i.get("Status") == "Continuing" and i.get("ProviderIds", {}).get("Tmdb")]
        except Exception as e:
            logger.error(f"Emby API è¯·æ±‚å¤±è´¥: {e}")
            return []
        return []

    def _fetch_series_status(self, series, api_key, start_date, end_date, proxies):
        """æŠ“å– TMDB æ•°æ®å¹¶å¯¹æ¯”æœ¬åœ°åº“å­˜"""
        tmdb_id = series.get("ProviderIds", {}).get("Tmdb")
        if not tmdb_id: return []

        try:
            # 1. æŠ“å–å‰§é›†åŸºæœ¬ä¿¡æ¯ï¼Œæå–å‰§é›†æ€»ç®€ä»‹ (series_overview) ç”¨äºå‰ç«¯å…œåº•
            url_series = f"https://api.themoviedb.org/3/tv/{tmdb_id}?api_key={api_key}&language=zh-CN"
            res_series = requests.get(url_series, timeout=5, proxies=proxies)
            if res_series.status_code != 200: return []
            
            data_series = res_series.json()
            series_overview = data_series.get("overview") 
            
            # 2. é”å®šç›®æ ‡å­£ï¼ˆæŠ“å–æœ€åæ’­å‡ºçš„å’Œä¸‹æ¬¡æ’­å‡ºçš„å­£ï¼‰
            target_seasons = set()
            if data_series.get("last_episode_to_air"):
                target_seasons.add(data_series["last_episode_to_air"].get("season_number"))
            if data_series.get("next_episode_to_air"):
                target_seasons.add(data_series["next_episode_to_air"].get("season_number"))
            if not target_seasons and data_series.get("seasons"):
                target_seasons.add(data_series["seasons"][-1].get("season_number"))

            final_episodes = []

            # 3. éå†ç›®æ ‡å­£ï¼Œç­›é€‰å‡ºæœ¬å‘¨æ›´æ–°çš„å•é›†
            for season_num in target_seasons:
                if season_num is None: continue
                url_season = f"https://api.themoviedb.org/3/tv/{tmdb_id}/season/{season_num}?api_key={api_key}&language=zh-CN"
                res_season = requests.get(url_season, timeout=5, proxies=proxies)
                if res_season.status_code != 200: continue
                
                episodes_list = res_season.json().get("episodes", [])
                for ep in episodes_list:
                    air_date_str = ep.get("air_date")
                    if not air_date_str: continue
                    
                    try:
                        air_date = datetime.datetime.strptime(air_date_str, "%Y-%m-%d").date()
                        if start_date <= air_date <= end_date:
                            # ğŸ”¥ ä¸¥æ ¼ç‰©ç†æ ¡éªŒï¼šå» Emby åŒ¹é…ç‰©ç†æ–‡ä»¶
                            has_file = self._check_emby_has_episode(series["Id"], ep["season_number"], ep["episode_number"])
                            
                            today = datetime.date.today()
                            status = "ready" if has_file else "missing" if air_date < today else "today" if air_date == today else "upcoming"

                            final_episodes.append({
                                "day_index": (air_date - start_date).days,
                                "data": {
                                    "series_name": series.get("Name"),
                                    "series_id": series.get("Id"),
                                    "tmdb_id": tmdb_id,
                                    "ep_name": ep.get("name"),
                                    "season": ep["season_number"],
                                    "episode": ep["episode_number"],
                                    "air_date": ep.get("air_date"),
                                    "poster_path": data_series.get("poster_path"),
                                    "status": status,
                                    "overview": ep.get("overview"),
                                    "series_overview": series_overview # ğŸ”¥ æ³¨å…¥å‰§é›†æ€»ç®€ä»‹
                                }
                            })
                    except: continue
            return final_episodes
        except: return []

    def _check_emby_has_episode(self, series_id, season, episode):
        """
        [æœ€ä¸¥æ ¼ç‰©ç†æ ¡éªŒ]
        æ‹‰å–è¯¥ç³»åˆ—æ‰€æœ‰é›†æ•°ï¼Œæ‰‹åŠ¨æ ¸å¯¹å­£å·ã€é›†å·ï¼Œå¹¶ç¡®ä¿ Path æˆ– MediaSources å­˜åœ¨
        ç»•è¿‡ Emby API æ— æ³•æŒ‰å­£é›†å·è¿‡æ»¤è™šæ‹Ÿå ä½ç¬¦çš„ Bug
        """
        key, host = cfg.get("emby_api_key"), cfg.get("emby_host")
        user_id = self._get_admin_id()
        if not key or not host or not user_id: return False
        
        try:
            url = f"{host}/emby/Users/{user_id}/Items"
            params = {
                "ParentId": series_id,
                "Recursive": "true",
                "IncludeItemTypes": "Episode",
                "Fields": "Path,MediaSources,LocationType", 
                "api_key": key
            }
            res = requests.get(url, params=params, timeout=5)
            if res.status_code == 200:
                items = res.json().get("Items", [])
                for item in items:
                    # 1. æ ¸å¯¹å­£å·å’Œé›†å·
                    if item.get("ParentIndexNumber") == season and item.get("IndexNumber") == episode:
                        # 2. è¿‡æ»¤è™šæ‹Ÿå’Œç¼ºå¤±æ ‡è®°
                        if item.get("LocationType", "") == "Virtual": continue
                        if item.get("IsMissing", False): continue
                        # 3. ç‰©ç†è·¯å¾„æ ¡éªŒï¼šå¿…é¡»æœ‰æ–‡ä»¶è·¯å¾„æˆ–åª’ä½“æµä¿¡æ¯
                        if item.get("Path") or item.get("MediaSources"):
                            return True
        except: pass
        return False

    def _get_admin_id(self):
        """è·å–ç¬¬ä¸€ä¸ªç®¡ç†å‘˜çš„ ID"""
        key, host = cfg.get("emby_api_key"), cfg.get("emby_host")
        try:
            res = requests.get(f"{host}/emby/Users?api_key={key}", timeout=3)
            if res.status_code == 200:
                users = res.json()
                return next((u['Id'] for u in users if u.get("Policy", {}).get("IsAdministrator")), users[0]['Id'])
        except: pass
        return None

# å•ä¾‹å®ä¾‹åŒ–
calendar_service = CalendarService()